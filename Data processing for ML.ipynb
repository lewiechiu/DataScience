{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning/ Prediction\n",
    "\n",
    "### Predicting using shallow-learning techniques\n",
    "* Random Forrest\n",
    "* XGBoost\n",
    "* SVR\n",
    "\n",
    "### Predicting using SOTA Machine-Learning NNs (blackbox though.....)\n",
    "* To play it fair with those previous shallow-learning methods, the NNs will be restricted to be shallow. Meaning it will have less/ equal to 3 layers. \n",
    "\n",
    "### Procedures\n",
    "We will first introduce RF, XGBoost, SVR techniques. While we are doing those tasks, we will time those methods. Making $time$ another factor it should consider, rather than taking indefinite amount of time and perform 2$\\%$ better. That way, we will have a index of $\\dfrac{accuracy}{time}$ (accuracy gain trained per minute). In my opinion, it can be a rough index upon how we should rapidly prototype some ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using SOTA Machine-Learning NNs (blackbox though.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "# df = pd.read_csv(\"data/train_V2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    # iterate through all the columns of a dataframe and modify the data type\n",
    "    #   to reduce memory usage.        \n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above technique is used when the ram explodes in 16GB. Basically, it casts different types of number between different variable type. For instance, when we have $705023$, we wouldn't need a $int64$ to store it, what we need at most is $int16$. This can save up to 48 bit. When we have, say 1 million rows, we save upto 48 Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureModify(isTrain):\n",
    "    if isTrain:\n",
    "        all_data = pd.read_csv(\"data/train_V2.csv\")\n",
    "        all_data = all_data[all_data['maxPlace'] > 1]\n",
    "        all_data = reduce_mem_usage(all_data)\n",
    "        all_data = all_data[all_data['winPlacePerc'].notnull()]\n",
    "    else:\n",
    "        all_data = pd.read_csv('../input/test_V2.csv')\n",
    "\n",
    "\n",
    "    all_data['matchType'] = all_data['matchType'].map({\n",
    "    'crashfpp':1,\n",
    "    'crashtpp':2,\n",
    "    'duo':3,\n",
    "    'duo-fpp':4,\n",
    "    'flarefpp':5,\n",
    "    'flaretpp':6,\n",
    "    'normal-duo':7,\n",
    "    'normal-duo-fpp':8,\n",
    "    'normal-solo':9,\n",
    "    'normal-solo-fpp':10,\n",
    "    'normal-squad':11,\n",
    "    'normal-squad-fpp':12,\n",
    "    'solo':13,\n",
    "    'solo-fpp':14,\n",
    "    'squad':15,\n",
    "    'squad-fpp':16\n",
    "    })\n",
    "    all_data = reduce_mem_usage(all_data)\n",
    "\n",
    "    print(\"Match size\")\n",
    "    matchSizeData = all_data.groupby(['matchId']).size().reset_index(name='matchSize')\n",
    "    all_data = pd.merge(all_data, matchSizeData, how='left', on=['matchId'])\n",
    "    del matchSizeData\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    all_data.loc[(all_data['rankPoints']==-1), 'rankPoints'] = 0\n",
    "    all_data['_killPoints_rankpoints'] = all_data['rankPoints']+all_data['killPoints']\n",
    "\n",
    "\n",
    "    all_data[\"_Kill_headshot_Ratio\"] = all_data[\"kills\"]/all_data[\"headshotKills\"]\n",
    "    all_data['_killStreak_Kill_ratio'] = all_data['killStreaks']/all_data['kills']\n",
    "    all_data['_totalDistance'] = 0.25*all_data['rideDistance'] + all_data[\"walkDistance\"] + all_data[\"swimDistance\"]\n",
    "    all_data['_killPlace_MaxPlace_Ratio'] = all_data['killPlace'] / all_data['maxPlace']\n",
    "    all_data['_totalDistance_weaponsAcq_Ratio'] = all_data['_totalDistance'] / all_data['weaponsAcquired']\n",
    "    all_data['_walkDistance_heals_Ratio'] = all_data['walkDistance'] / all_data['heals']\n",
    "    all_data['_walkDistance_kills_Ratio'] = all_data['walkDistance'] / all_data['kills']\n",
    "    all_data['_kills_walkDistance_Ratio'] = all_data['kills'] / all_data['walkDistance']\n",
    "    all_data['_totalDistancePerDuration'] =  all_data[\"_totalDistance\"]/all_data[\"matchDuration\"]\n",
    "    all_data['_killPlace_kills_Ratio'] = all_data['killPlace']/all_data['kills']\n",
    "    all_data['_walkDistancePerDuration'] =  all_data[\"walkDistance\"]/all_data[\"matchDuration\"]\n",
    "    all_data['walkDistancePerc'] = all_data.groupby('matchId')['walkDistance'].rank(pct=True).values\n",
    "    all_data['killPerc'] = all_data.groupby('matchId')['kills'].rank(pct=True).values\n",
    "    all_data['killPlacePerc'] = all_data.groupby('matchId')['killPlace'].rank(pct=True).values\n",
    "    all_data['weaponsAcquired'] = all_data.groupby('matchId')['weaponsAcquired'].rank(pct=True).values\n",
    "    all_data['_walkDistance_kills_Ratio2'] = all_data['walkDistancePerc'] / all_data['killPerc']\n",
    "    all_data['_kill_kills_Ratio2'] = all_data['killPerc']/all_data['walkDistancePerc']\n",
    "    all_data['_killPlace_walkDistance_Ratio2'] = all_data['walkDistancePerc']/all_data['killPlacePerc']\n",
    "    all_data['_killPlace_kills_Ratio2'] = all_data['killPlacePerc']/all_data['killPerc']\n",
    "    all_data['_totalDistance'] = all_data.groupby('matchId')['_totalDistance'].rank(pct=True).values\n",
    "    all_data['_walkDistance_kills_Ratio3'] = all_data['walkDistancePerc'] / all_data['kills']\n",
    "    all_data['_walkDistance_kills_Ratio4'] = all_data['kills'] / all_data['walkDistancePerc']\n",
    "    all_data['_walkDistance_kills_Ratio5'] = all_data['killPerc'] / all_data['walkDistance']\n",
    "    all_data['_walkDistance_kills_Ratio6'] = all_data['walkDistance'] / all_data['killPerc']\n",
    "\n",
    "    all_data[all_data == np.Inf] = np.NaN\n",
    "    all_data[all_data == np.NINF] = np.NaN\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    \n",
    "    features = list(all_data.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchSize\")\n",
    "    features.remove(\"matchType\")\n",
    "    if isTrain:\n",
    "        features.remove(\"winPlacePerc\")\n",
    "\n",
    "    \n",
    "    print(\"Mean Data\")\n",
    "    meanData = all_data.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    meanData = reduce_mem_usage(meanData)\n",
    "    meanData = meanData.replace([np.inf, np.NINF,np.nan], 0)\n",
    "    meanDataRank = meanData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    meanDataRank = reduce_mem_usage(meanDataRank)\n",
    "    all_data = pd.merge(all_data, meanData.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n",
    "    del meanData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"vehicleDestroys_mean\",\"rideDistance_mean\",\"roadKills_mean\",\"rankPoints_mean\"], axis=1)\n",
    "    all_data = pd.merge(all_data, meanDataRank, suffixes=[\"\", \"_meanRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del meanDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"numGroups_meanRank\",\"rankPoints_meanRank\"], axis=1)\n",
    "    \n",
    "    all_data = all_data.join(reduce_mem_usage(all_data.groupby('matchId')[features].rank(ascending=False).add_suffix('_rankPlace').astype(int)))\n",
    "\n",
    "    \n",
    "    print(\"Std Data\")\n",
    "    stdData = all_data.groupby(['matchId','groupId'])[features].agg('std').replace([np.inf, np.NINF,np.nan], 0)\n",
    "    stdDataRank = reduce_mem_usage(stdData.groupby('matchId')[features].rank(pct=True)).reset_index()\n",
    "    del stdData\n",
    "    gc.collect()\n",
    "    all_data = pd.merge(all_data, stdDataRank, suffixes=[\"\", \"_stdRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del stdDataRank\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Max Data\")\n",
    "    maxData = all_data.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    maxData = reduce_mem_usage(maxData)\n",
    "    maxDataRank = maxData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    maxDataRank = reduce_mem_usage(maxDataRank)\n",
    "    all_data = pd.merge(all_data, maxData.reset_index(), suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\n",
    "    del maxData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"assists_max\",\"killPoints_max\",\"headshotKills_max\",\"numGroups_max\",\"revives_max\",\"teamKills_max\",\"roadKills_max\",\"vehicleDestroys_max\"], axis=1)\n",
    "    all_data = pd.merge(all_data, maxDataRank, suffixes=[\"\", \"_maxRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del maxDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"roadKills_maxRank\",\"matchDuration_maxRank\",\"maxPlace_maxRank\",\"numGroups_maxRank\"], axis=1)\n",
    "\n",
    "\n",
    "    print(\"Min Data\")\n",
    "    minData = all_data.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    minData = reduce_mem_usage(minData)\n",
    "    minDataRank = minData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    minDataRank = reduce_mem_usage(minDataRank)\n",
    "    all_data = pd.merge(all_data, minData.reset_index(), suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\n",
    "    del minData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"heals_min\",\"killStreaks_min\",\"killPoints_min\",\"maxPlace_min\",\"revives_min\",\"headshotKills_min\",\"weaponsAcquired_min\",\"_walkDistance_kills_Ratio_min\",\"rankPoints_min\",\"matchDuration_min\",\"teamKills_min\",\"numGroups_min\",\"assists_min\",\"roadKills_min\",\"vehicleDestroys_min\"], axis=1)\n",
    "    all_data = pd.merge(all_data, minDataRank, suffixes=[\"\", \"_minRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del minDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"killPoints_minRank\",\"matchDuration_minRank\",\"maxPlace_minRank\",\"numGroups_minRank\"], axis=1)\n",
    "\n",
    "    \n",
    "    print(\"group Size\")\n",
    "    groupSize = all_data.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "    groupSize = reduce_mem_usage(groupSize)\n",
    "    all_data = pd.merge(all_data, groupSize, how='left', on=['matchId', 'groupId'])\n",
    "    del groupSize\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    print(\"Match Mean\")\n",
    "    matchMeanFeatures = features\n",
    "    matchMeanFeatures = [ v for v in matchMeanFeatures if v not in [\"killPlacePerc\",\"matchDuration\",\"maxPlace\",\"numGroups\"] ]\n",
    "    matchMeanData= reduce_mem_usage(all_data.groupby(['matchId'])[matchMeanFeatures].transform('mean')).replace([np.inf, np.NINF,np.nan], 0)\n",
    "    all_data = pd.concat([all_data,matchMeanData.add_suffix('_matchMean')],axis=1)\n",
    "    del matchMeanData,matchMeanFeatures\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"matchMax\")\n",
    "    matchMaxFeatures = [\"walkDistance\",\"kills\",\"_walkDistance_kills_Ratio\",\"_kill_kills_Ratio2\"]\n",
    "    all_data = pd.merge(all_data, reduce_mem_usage(all_data.groupby(['matchId'])[matchMaxFeatures].agg('max')).reset_index(), suffixes=[\"\", \"_matchMax\"], how='left', on=['matchId'])\n",
    "\n",
    "    print(\"match STD\")\n",
    "    matchMaxFeatures = [\"kills\",\"_walkDistance_kills_Ratio2\",\"_walkDistance_kills_Ratio\",\"killPerc\",\"_kills_walkDistance_Ratio\"]\n",
    "    all_data = pd.merge(all_data, reduce_mem_usage(all_data.groupby(['matchId'])[matchMaxFeatures].agg('std')).reset_index().replace([np.inf, np.NINF,np.nan], 0), suffixes=[\"\", \"_matchSTD\"], how='left', on=['matchId'])\n",
    "\n",
    "\n",
    "    all_data = all_data.drop([\"Id\",\"groupId\"], axis=1)\n",
    "    all_data = all_data.drop([\"DBNOs\",\"assists\",\"headshotKills\",\"heals\",\"killPoints\",\"_killStreak_Kill_ratio\",\"killStreaks\",\"longestKill\",\"revives\",\"roadKills\",\"teamKills\",\"vehicleDestroys\",\"_walkDistance_kills_Ratio\",\"weaponsAcquired\"], axis=1)\n",
    "    all_data = all_data.drop([\"_walkDistance_heals_Ratio\",\"_totalDistancePerDuration\",\"_killPlace_kills_Ratio\",\"_totalDistance_weaponsAcq_Ratio\",\"_killPlace_MaxPlace_Ratio\",\"_walkDistancePerDuration\",\"rankPoints\",\"rideDistance\",\"boosts\",\"winPoints\",\"swimDistance\",\"_kills_walkDistance_Ratio\"], axis=1)\n",
    "    all_data = all_data.drop([\"_Kill_headshot_Ratio\",\"maxPlace\",\"_totalDistance\",\"numGroups\",\"walkDistance\",\"killPlace\"], axis=1)\n",
    "    all_data = reduce_mem_usage(all_data)\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"done\")\n",
    "    features_label = all_data.columns\n",
    "    features_label = features_label.drop('matchId')\n",
    "    if isTrain:\n",
    "        features_label = features_label.drop('winPlacePerc')\n",
    "\n",
    "    gc.collect()\n",
    "    return all_data,features_label\n",
    "\n",
    "\n",
    "def split_train_val(data, fraction):\n",
    "    matchIds = data['matchId'].unique().reshape([-1])\n",
    "    train_size = int(len(matchIds)*fraction)\n",
    "    \n",
    "    random_idx = np.random.RandomState(seed=2).permutation(len(matchIds))\n",
    "    train_matchIds = matchIds[random_idx[:train_size]]\n",
    "    val_matchIds = matchIds[random_idx[train_size:]]\n",
    "    \n",
    "    data_train = data.loc[data['matchId'].isin(train_matchIds)]\n",
    "    data_val = data.loc[data['matchId'].isin(val_matchIds)]\n",
    "    return data_train, data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1017.83 MB\n",
      "Memory usage after optimization is: 322.31 MB\n",
      "Decreased by 68.3%\n",
      "Memory usage of dataframe is 322.31 MB\n",
      "Memory usage after optimization is: 292.63 MB\n",
      "Decreased by 9.2%\n",
      "Match size\n",
      "Mean Data\n",
      "Memory usage of dataframe is 642.07 MB\n",
      "Memory usage after optimization is: 216.85 MB\n",
      "Decreased by 66.2%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "Memory usage of dataframe is 1628.53 MB\n",
      "Memory usage after optimization is: 233.25 MB\n",
      "Decreased by 85.7%\n",
      "Std Data\n",
      "Memory usage of dataframe is 758.04 MB\n",
      "Memory usage after optimization is: 212.98 MB\n",
      "Decreased by 71.9%\n",
      "Max Data\n",
      "Memory usage of dataframe is 394.67 MB\n",
      "Memory usage after optimization is: 189.79 MB\n",
      "Decreased by 51.9%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "Min Data\n",
      "Memory usage of dataframe is 394.67 MB\n",
      "Memory usage after optimization is: 189.79 MB\n",
      "Decreased by 51.9%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "group Size\n",
      "Memory usage of dataframe is 46.39 MB\n",
      "Memory usage after optimization is: 32.86 MB\n",
      "Decreased by 29.2%\n",
      "Match Mean\n",
      "Memory usage of dataframe is 1238.36 MB\n",
      "Memory usage after optimization is: 398.65 MB\n",
      "Decreased by 67.8%\n",
      "matchMax\n",
      "Memory usage of dataframe is 0.96 MB\n",
      "Memory usage after optimization is: 0.69 MB\n",
      "Decreased by 28.6%\n",
      "match STD\n",
      "Memory usage of dataframe is 2.20 MB\n",
      "Memory usage after optimization is: 1.10 MB\n",
      "Decreased by 50.0%\n",
      "Memory usage of dataframe is 3638.74 MB\n",
      "Memory usage after optimization is: 3312.19 MB\n",
      "Decreased by 9.0%\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "X_train,features_label = featureModify(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y time\n",
      "X test np time\n",
      "y test np time\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train_test = split_train_val(X_train, 0.91)\n",
    "print(\"Y time\")\n",
    "y = X_train['winPlacePerc']\n",
    "y_test = X_train_test['winPlacePerc']\n",
    "X_train = X_train.drop(columns=['matchId', 'winPlacePerc'])\n",
    "X_train_test = X_train_test.drop(columns=['matchId', 'winPlacePerc'])\n",
    "\n",
    "print(\"X test np time\")\n",
    "X_train_test = np.array(X_train_test)\n",
    "print(\"y test np time\")\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "y = np.array(y)\n",
    "X_train = np.array(X_train)\n",
    "np.save(\"y\", y)\n",
    "np.save(\"x\", X_train)\n",
    "np.save(\"x_test\",X_train_test)\n",
    "np.save(\"y_test\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4044887, 409)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the file, so the next time I don't have to spend time waiting.\n",
    "X_train = np.load(\"x.npy\", allow_pickle=True)\n",
    "X_train_test = np.load(\"x_test.npy\", allow_pickle=True)\n",
    "y = np.load('y.npy', allow_pickle=True)\n",
    "y_test = np.load('y_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 450)               184500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 450)               202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 450)               202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 451       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 596,251\n",
      "Trainable params: 593,551\n",
      "Non-trainable params: 2,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(450, input_dim=409))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(450))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(450))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4044887 samples, validate on 402078 samples\n",
      "Epoch 1/40\n",
      "4044887/4044887 [==============================] - 88s 22us/step - loss: 0.0101 - mean_absolute_error: 0.0598 - val_loss: 0.0065 - val_mean_absolute_error: 0.0643\n",
      "Epoch 2/40\n",
      "4044887/4044887 [==============================] - 89s 22us/step - loss: 0.0028 - mean_absolute_error: 0.0391 - val_loss: 0.0042 - val_mean_absolute_error: 0.0506\n",
      "Epoch 3/40\n",
      "4044887/4044887 [==============================] - 86s 21us/step - loss: 0.0024 - mean_absolute_error: 0.0359 - val_loss: 0.0041 - val_mean_absolute_error: 0.0492\n",
      "Epoch 4/40\n",
      "4044887/4044887 [==============================] - 83s 21us/step - loss: 0.0021 - mean_absolute_error: 0.0339 - val_loss: 0.0049 - val_mean_absolute_error: 0.0547\n",
      "Epoch 5/40\n",
      "4044887/4044887 [==============================] - 86s 21us/step - loss: 0.0020 - mean_absolute_error: 0.0326 - val_loss: 0.0026 - val_mean_absolute_error: 0.0359\n",
      "Epoch 6/40\n",
      "4044887/4044887 [==============================] - 86s 21us/step - loss: 0.0019 - mean_absolute_error: 0.0313 - val_loss: 0.0024 - val_mean_absolute_error: 0.0362\n",
      "Epoch 7/40\n",
      "4044887/4044887 [==============================] - 83s 21us/step - loss: 0.0018 - mean_absolute_error: 0.0304 - val_loss: 0.0075 - val_mean_absolute_error: 0.0676\n",
      "Epoch 8/40\n",
      "4044887/4044887 [==============================] - 89s 22us/step - loss: 0.0017 - mean_absolute_error: 0.0297 - val_loss: 0.0037 - val_mean_absolute_error: 0.0476\n",
      "Epoch 9/40\n",
      "4044887/4044887 [==============================] - 85s 21us/step - loss: 0.0017 - mean_absolute_error: 0.0293 - val_loss: 0.0064 - val_mean_absolute_error: 0.0592\n",
      "Epoch 10/40\n",
      "4044887/4044887 [==============================] - 87s 21us/step - loss: 0.0016 - mean_absolute_error: 0.0287 - val_loss: 0.0110 - val_mean_absolute_error: 0.0885\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "es = EarlyStopping(patience=4)\n",
    "model.fit(X_train,y, validation_data=(X_train_test,y_test), epochs=40, batch_size=2048, callbacks=[es])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results seems to have exploded in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863.3746693134308, 0.0011166646813563195)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start, (1 - 0.0359)/ (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took us 863.37 seconds to reach a best mae of 0.0359. That is, for every 1 addtional second we train the model, the model roughly gives us a 0.0011 mae improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/louiechiu/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 450)               184500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 450)               202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 450)               202950    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 450)               1800      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 451       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 596,251\n",
      "Trainable params: 593,551\n",
      "Non-trainable params: 2,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.save('450_3.h5')\n",
    "# del model\n",
    "model = Sequential()\n",
    "model.add(Dense(450, input_dim=409))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(450))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(450))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/louiechiu/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4044887 samples, validate on 402078 samples\n",
      "Epoch 1/80\n",
      "4044887/4044887 [==============================] - 88s 22us/step - loss: 0.0600 - mean_absolute_error: 0.1734 - val_loss: 0.0379 - val_mean_absolute_error: 0.1418\n",
      "Epoch 2/80\n",
      "4044887/4044887 [==============================] - 96s 24us/step - loss: 0.0189 - mean_absolute_error: 0.1005 - val_loss: 0.0135 - val_mean_absolute_error: 0.0831\n",
      "Epoch 3/80\n",
      "4044887/4044887 [==============================] - 91s 22us/step - loss: 0.0137 - mean_absolute_error: 0.0845 - val_loss: 0.0119 - val_mean_absolute_error: 0.0817\n",
      "Epoch 4/80\n",
      "4044887/4044887 [==============================] - 77s 19us/step - loss: 0.0103 - mean_absolute_error: 0.0727 - val_loss: 0.0186 - val_mean_absolute_error: 0.1093\n",
      "Epoch 5/80\n",
      "4044887/4044887 [==============================] - 81s 20us/step - loss: 0.0081 - mean_absolute_error: 0.0644 - val_loss: 0.0107 - val_mean_absolute_error: 0.0810\n",
      "Epoch 6/80\n",
      "4044887/4044887 [==============================] - 83s 21us/step - loss: 0.0068 - mean_absolute_error: 0.0589 - val_loss: 0.0066 - val_mean_absolute_error: 0.0618\n",
      "Epoch 7/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0060 - mean_absolute_error: 0.0553 - val_loss: 0.0060 - val_mean_absolute_error: 0.0583\n",
      "Epoch 8/80\n",
      "4044887/4044887 [==============================] - 81s 20us/step - loss: 0.0054 - mean_absolute_error: 0.0528 - val_loss: 0.0055 - val_mean_absolute_error: 0.0556\n",
      "Epoch 9/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0050 - mean_absolute_error: 0.0507 - val_loss: 0.0039 - val_mean_absolute_error: 0.0467\n",
      "Epoch 10/80\n",
      "4044887/4044887 [==============================] - 84s 21us/step - loss: 0.0047 - mean_absolute_error: 0.0491 - val_loss: 0.0044 - val_mean_absolute_error: 0.0498\n",
      "Epoch 11/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0043 - mean_absolute_error: 0.0475 - val_loss: 0.0037 - val_mean_absolute_error: 0.0467\n",
      "Epoch 12/80\n",
      "4044887/4044887 [==============================] - 83s 21us/step - loss: 0.0041 - mean_absolute_error: 0.0463 - val_loss: 0.0040 - val_mean_absolute_error: 0.0469\n",
      "Epoch 13/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0039 - mean_absolute_error: 0.0451 - val_loss: 0.0033 - val_mean_absolute_error: 0.0434\n",
      "Epoch 14/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0037 - mean_absolute_error: 0.0442 - val_loss: 0.0032 - val_mean_absolute_error: 0.0431\n",
      "Epoch 15/80\n",
      "4044887/4044887 [==============================] - 83s 20us/step - loss: 0.0036 - mean_absolute_error: 0.0432 - val_loss: 0.0030 - val_mean_absolute_error: 0.0405\n",
      "Epoch 16/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0035 - mean_absolute_error: 0.0426 - val_loss: 0.0031 - val_mean_absolute_error: 0.0417\n",
      "Epoch 17/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0033 - mean_absolute_error: 0.0418 - val_loss: 0.0031 - val_mean_absolute_error: 0.0421\n",
      "Epoch 18/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0032 - mean_absolute_error: 0.0412 - val_loss: 0.0029 - val_mean_absolute_error: 0.0395\n",
      "Epoch 19/80\n",
      "4044887/4044887 [==============================] - 79s 20us/step - loss: 0.0031 - mean_absolute_error: 0.0407 - val_loss: 0.0069 - val_mean_absolute_error: 0.0623\n",
      "Epoch 20/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0030 - mean_absolute_error: 0.0401 - val_loss: 0.0036 - val_mean_absolute_error: 0.0455\n",
      "Epoch 21/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0030 - mean_absolute_error: 0.0397 - val_loss: 0.0029 - val_mean_absolute_error: 0.0420\n",
      "Epoch 22/80\n",
      "4044887/4044887 [==============================] - 84s 21us/step - loss: 0.0029 - mean_absolute_error: 0.0392 - val_loss: 0.0027 - val_mean_absolute_error: 0.0394\n",
      "Epoch 23/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0028 - mean_absolute_error: 0.0385 - val_loss: 0.0027 - val_mean_absolute_error: 0.0383\n",
      "Epoch 24/80\n",
      "4044887/4044887 [==============================] - 82s 20us/step - loss: 0.0028 - mean_absolute_error: 0.0382 - val_loss: 0.0034 - val_mean_absolute_error: 0.0455\n",
      "Epoch 25/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0028 - mean_absolute_error: 0.0383 - val_loss: 0.0039 - val_mean_absolute_error: 0.0503\n",
      "Epoch 26/80\n",
      "4044887/4044887 [==============================] - 76s 19us/step - loss: 0.0027 - mean_absolute_error: 0.0379 - val_loss: 0.0033 - val_mean_absolute_error: 0.0459\n",
      "Epoch 27/80\n",
      "4044887/4044887 [==============================] - 77s 19us/step - loss: 0.0027 - mean_absolute_error: 0.0375 - val_loss: 0.0030 - val_mean_absolute_error: 0.0429\n",
      "Epoch 28/80\n",
      "4044887/4044887 [==============================] - 76s 19us/step - loss: 0.0026 - mean_absolute_error: 0.0370 - val_loss: 0.0034 - val_mean_absolute_error: 0.0436\n",
      "Epoch 29/80\n",
      "4044887/4044887 [==============================] - 73s 18us/step - loss: 0.0025 - mean_absolute_error: 0.0366 - val_loss: 0.0032 - val_mean_absolute_error: 0.0423\n",
      "Epoch 30/80\n",
      "4044887/4044887 [==============================] - 77s 19us/step - loss: 0.0025 - mean_absolute_error: 0.0365 - val_loss: 0.0033 - val_mean_absolute_error: 0.0452\n",
      "Epoch 31/80\n",
      "4044887/4044887 [==============================] - 80s 20us/step - loss: 0.0025 - mean_absolute_error: 0.0365 - val_loss: 0.0031 - val_mean_absolute_error: 0.0429\n",
      "Epoch 32/80\n",
      "4044887/4044887 [==============================] - 76s 19us/step - loss: 0.0025 - mean_absolute_error: 0.0361 - val_loss: 0.0024 - val_mean_absolute_error: 0.0370\n",
      "Epoch 33/80\n",
      "4044887/4044887 [==============================] - 72s 18us/step - loss: 0.0024 - mean_absolute_error: 0.0357 - val_loss: 0.0028 - val_mean_absolute_error: 0.0412\n",
      "Epoch 34/80\n",
      "4044887/4044887 [==============================] - 74s 18us/step - loss: 0.0024 - mean_absolute_error: 0.0355 - val_loss: 0.0029 - val_mean_absolute_error: 0.0423\n",
      "Epoch 35/80\n",
      "4044887/4044887 [==============================] - 76s 19us/step - loss: 0.0024 - mean_absolute_error: 0.0353 - val_loss: 0.0037 - val_mean_absolute_error: 0.0505\n",
      "Epoch 36/80\n",
      "4044887/4044887 [==============================] - 75s 19us/step - loss: 0.0023 - mean_absolute_error: 0.0348 - val_loss: 0.0027 - val_mean_absolute_error: 0.0399\n",
      "Epoch 37/80\n",
      "4044887/4044887 [==============================] - 79s 19us/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0037 - val_mean_absolute_error: 0.0495\n",
      "Epoch 38/80\n",
      "4044887/4044887 [==============================] - 75s 19us/step - loss: 0.0022 - mean_absolute_error: 0.0344 - val_loss: 0.0032 - val_mean_absolute_error: 0.0447\n",
      "Epoch 39/80\n",
      "4044887/4044887 [==============================] - 75s 18us/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0028 - val_mean_absolute_error: 0.0422\n",
      "Epoch 40/80\n",
      "4044887/4044887 [==============================] - 73s 18us/step - loss: 0.0022 - mean_absolute_error: 0.0342 - val_loss: 0.0029 - val_mean_absolute_error: 0.0419\n",
      "Epoch 41/80\n",
      "4044887/4044887 [==============================] - 71s 18us/step - loss: 0.0022 - mean_absolute_error: 0.0345 - val_loss: 0.0036 - val_mean_absolute_error: 0.0473\n",
      "Epoch 42/80\n",
      "4044887/4044887 [==============================] - 72s 18us/step - loss: 0.0022 - mean_absolute_error: 0.0338 - val_loss: 0.0036 - val_mean_absolute_error: 0.0497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3343.470095872879, 0.00028554165960044473)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "es = EarlyStopping(patience=10)\n",
    "model.fit(X_train,y, validation_data=(X_train_test,y_test), epochs=80, batch_size=30000, callbacks=[es])\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3343.470095872879, 0.000288024110396175)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start, (1 - 0.037)/ (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From there we see if we apply some more dropout layers to the neural network, although it did perform somewhat better "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
